---
title: "MATH 3190 Final Project"
author: "Jun Hanvey, Ian McFarlane, Kellen Nankervis"
date: "Due April 25, 2024"
output: 
 #beamer_presentation:
 #  theme: CambridgeUS
 pdf_document:
header-includes:
  - \usepackage{amsmath}
  - \usepackage{algorithm}
  - \newcommand{\Ab}{{\mathbf A}}
  - \newcommand{\Xb}{{\mathbf X}}
  - \newcommand{\xb}{\boldsymbol{x}}
  - \newcommand{\yb}{\boldsymbol{y}}
  - \newcommand{\bb}{\boldsymbol{b}}
  - \newcommand{\R}{\mathbb{R}}
  - \newcommand{\rb}{\boldsymbol{r}}
  - \newcommand{\ub}{\boldsymbol{u}}
  - \newcommand{\vb}{\boldsymbol{v}}
  - \newcommand{\pb}{\boldsymbol{p}}
  - \newcommand{\Ib}{{\mathbf I}}
  - \makeatletter
  - \preto{\@verbatim}{\topsep=-10pt \partopsep=4pt }
  - \makeatother
editor_options: 
  chunk_output_type: console
urlcolor: blue
---

## Introduction

In Notes 11, we explored Support Vector Machines, or SVMs, which are highly powerful classifiers. To put it briefly, they work by finding a linear hyperplane which maximizes the margin between two classes, called the decision boundary. But what happens if our data are not linearly separable and therefore a linear decision boundary cannot be formed? We can use a kernel trick, which allows us to transform the data into a higher dimension, becoming linearly separable, without actually transforming the data. The RBF Kernel in particular finds an infinite-dimensional projection of the data. In this project we will explore the Radial Basis Function Kernel and how it is an extremely powerful tool for classifying data that could not be classified with a linear kernel or other data with a extremely curved decision boundary.


Consider the following non-linearly separable data. Observe how the best SVM can do is classify everything as the majority class (purple).

```{python, message=FALSE, warning=FALSE, echo=F, cache=T, results='hide',fig.keep='all'}
import matplotlib
import matplotlib.pyplot as plt
matplotlib.use('Agg')
import matplotlib.colors as clr
import numpy as np
from sklearn import svm
from sklearn.inspection import DecisionBoundaryDisplay



def plot_with_boundary(kernel, kernel_label=None):
    if kernel_label is None:
      kernel_label = kernel

    # Train the SVC
    clf = svm.SVC(kernel=kernel, gamma=1).fit(X, y)

    # Settings for plotting
    _, ax = plt.subplots(figsize=(3.2, 3))
    x_min, x_max, y_min, y_max = -3, 3, -3, 3
    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))

    # Plot decision boundary and margins
    common_params = {"estimator": clf, "X": X, "ax": ax}
    DecisionBoundaryDisplay.from_estimator(
        **common_params,
        response_method="predict",
        plot_method="pcolormesh",
        alpha=0.3,
    )
    DecisionBoundaryDisplay.from_estimator(
        **common_params,
        response_method="decision_function",
        plot_method="contour",
        levels=[-1, 0, 1],
        colors=["k", "k", "k"],
        linestyles=["--", "-", "--"],
    )

    
      # Plot bigger circles around samples that serve as support vectors
    ax.scatter(
        X[clf.support_][:, 0],
        X[clf.support_][:, 1],
        s=250,
        facecolors="none",
        edgecolors="k",
    )
    # Plot samples by color and add legend
    ax.scatter(X[:, 0], X[:, 1], c=y, s=150, edgecolors="k")
    ax.legend(*scatter.legend_elements(), loc="upper right", title="Classes")
    ax.set_title(f"Decision boundaries of\n{kernel_label} Kernel in SVC", fontsize=10)

    _ = plt.show()
    
def power_kernel(p):
    return lambda A, B: (A @ B.T) ** p

def poly_kernel(p):
    return lambda A, B: (A @ B.T + 1 ) ** p

template = np.array(
    [
        [1, 1],
        [1.1, 1.1],
        [0.9, 0.9],
        [1.1, 0.9],
        [0.9, 1.1]
    ]
)

X = np.vstack(
    [
        template + np.array([-3,-3]), template + np.array([-1,-3]), template + np.array([1,-3]),
        template + np.array([-3,-1]), template + np.array([-1,-1]), template + np.array([1,-1]),
        template + np.array([-3,1]), template + np.array([-1,1]), template + np.array([1,1])
    ]
)

y = np.array(([0] * 5 + [1] * 5) * 4 + [0] * 5)


fig, ax = plt.subplots(figsize=(3.2, 3))
x_min, x_max, y_min, y_max = -3, 3, -3, 3
ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))

scatter = ax.scatter(X[:, 0], X[:, 1], s=150, c=y, label=y, edgecolors="k")
ax.legend(*scatter.legend_elements(), loc="upper right", title="Classes")
ax.set_title("Samples in two-dimensional\nfeature space", fontsize=10)
_ = plt.show()

plot_with_boundary("linear")
```

There are some workarounds by using other **Kernels**. But, in order to understand what a Kernel is, we need to dive into the math for SVMs. 

## Mathematics behind SVM
We'll now dive into the mathematics behind SVM's. The mathematics can be a little bit advanced, but understanding the results is sufficient. It's important to note that we'll be focusing on the hard margin case as it's easier to follow. Nonetheless, the insights we gain will also apply to the soft margin case.

### Dual Problem

For the Support Vector Machine algorithm, our goal is to find a $\beta$ and $c$ under the following optimization objective:\begin{align*}
\underset{\boldsymbol{\beta}, c}{\text{minimize }} & \dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2_2\\
\text{subject to } & y_i(\boldsymbol{\beta} \cdot\xb_i-c)\ge 1 \text{ for all } i
\end{align*}

Then, we can find the [dual optimization problem](https://www.youtube.com/watch?v=uh1Dk68cfWs) by the use Lagrange Multipliers:\begin{align*}
\underset{\boldsymbol{\lambda}}{\text{maximize }}\underset{\boldsymbol{\beta}, c}{\text{minimize }} 
L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) = \dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2_2 - \sum_{i=1}^n \lambda_i \Big( y_i(\boldsymbol{\beta} \cdot\xb_i-c) - 1 \Big) 
\end{align*}

The dual problem will be satisfied when all partial derivatives are zero, Which leads us to the following results:\begin{align*}
\dfrac{\partial{L}}{\partial{\boldsymbol{\beta}}} & 
= \boldsymbol{\beta} - \sum_{i=1}^n \lambda_i y_i \xb_i \overset{\text{set}}{=} 0 \iff
\boldsymbol{\beta} = \sum_{i=1}^n \lambda_i y_i \xb_i \\
\dfrac{\partial{L}}{\partial{\boldsymbol{\lambda}}} & = \sum_{i=1}^n y_i(\boldsymbol{\beta} \cdot\xb_i-c) - 1 \overset{\text{set}}{=} 0 \iff c = 
\dfrac{\sum_{i=1}^n y_i \boldsymbol{\beta} \cdot\xb_i-n}{\sum_{i=1}^n y_i} \\
\dfrac{\partial{L}}{\partial{c}} & = \sum_{i=1}^n \lambda_i y_i \overset{\text{set}}{=} 0 \iff \boldsymbol{\lambda} \cdot \boldsymbol{y} = 0
\end{align*}

Observe how $\boldsymbol{\beta}$ and $c$ can be derived from $\boldsymbol{\lambda}$, $\boldsymbol{y}$ and $X$. Substituting these results into the Lagrangian should lead to some nice results. Although, substituting for $c$ won't be necessary (it gets multiplied by zero). Then, replacing $\boldsymbol{\beta}$ in our Lagrangian yields the following results:\begin{align*}
L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) &= \dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2_2 - \sum_{i=1}^n \lambda_i \Big( y_i(\boldsymbol{\beta} \cdot\xb_i-c) - 1 \Big)  \\
&= \dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2_2 - \sum_{i=1}^n \lambda_i y_i\boldsymbol{\beta} \cdot\xb_i + c \sum_{i=1}^n \lambda_iy_i + \sum_{i=1}^n \lambda_i \\
&= \dfrac{1}{2}\Vert\sum_{j=1}^n \lambda_j y_j \xb_j\Vert^2_2  
- \sum_{i=1}^n \left( \sum_{j=1}^n \lambda_j y_j \xb_j\right) \cdot\Big(\lambda_i y_i\xb_i \Big) 
+ c \cdot 0 + \sum_{i=1}^n \lambda_i \\
&= \dfrac{1}{2} \left( \sum_{i=1}^n \lambda_i y_i \xb_i\right) \cdot \left( \sum_{j=1}^n \lambda_j y_j \xb_j\right) 
- \sum_{i=1}^n  \sum_{j=1}^n \left(\lambda_j y_j \xb_j\right) \cdot\left(\lambda_i y_i\xb_i\right) + \sum_{i=1}^n \lambda_i\\
&= \dfrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n (\lambda_i y_i \xb_i) \cdot(\lambda_j y_j \xb_j) - \sum_{i=1}^n \sum_{j=1}^n (\lambda_i y_i \xb_i) \cdot(\lambda_j y_j \xb_j) +  \sum_{i=1}^n \lambda_i\\
&= -\dfrac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \lambda_i\lambda_jy_iy_j\xb_i \cdot\xb_j + \sum_{i=1}^n\lambda_i\\
L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) &= \sum_{i=1}^n\lambda_i-\dfrac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \lambda_i\lambda_jy_iy_j\xb_i \cdot\xb_j
\end{align*}


This shows a version of the Lagrangian that doesn't depend on $\boldsymbol{\beta}$ nor $c$, which means the optimization problem reduces to:\begin{align*}
\underset{\boldsymbol{\lambda}}{\text{maximize }}L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) &= \sum_{i=1}^n\lambda_i-\dfrac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \lambda_i\lambda_jy_iy_j\xb_i \cdot\xb_j 
\end{align*}

### Kernel Trick

But, why did we go through all that trouble? In this form, we can observe that the Lagrangian depends  on 3 components only:

- $\lambda_i$, an optimization artifact 
- $y_i$, a fixed "binary" variable 
- $\xb_i$, the features we're using to predict the class


Notice, $\xb_i$ is the only aspect of our model we can modify (via feature engineering). So, let $\phi(\xb)$ be our feature engineer transformation, then our Lagrangian becomes:\begin{align*}
L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) &= \sum_{i=1}^n\lambda_i -\dfrac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \lambda_i\lambda_jy_iy_j\phi(\xb_i) \cdot\phi(\xb_j) 
\end{align*}

From this we can observe that we don't really need to calculate $\phi(\xb)$, a function that finds $\phi(\xb_i) \cdot\phi(\xb_j)$ from $\xb_i$ and $\xb_j$ is good enough. We call that function a Kernel and denote it by $K(\xb_i, \xb_j)$. To avoid having complicated sub-indices, we'll relabel the arguments of $K$ to $\boldsymbol{a}$ and $\boldsymbol{b}$, so we'll have $K(\boldsymbol{a}, \boldsymbol{b})$. Finding the $\phi(\boldsymbol{a}) \cdot\phi(\boldsymbol{b})$, without having to calculate $\phi$  is what we call the **Kernel Trick**.

![](figs/kernel_trick2.png)

One of the simplest transformations we can investigate is $K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = (\boldsymbol{a} \cdot \boldsymbol{b})^2$

If we expand the dot product, we get: 

$K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = (a_1b_1 + a_2b_2 + \cdots + a_nb_n)^2$

Expanding the square, gives:\begin{align*}
K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = & (a_1b_1)(a_1b_1) + (a_1b_1)(a_2b_2) + \cdots + (a_1b_1)(a_nb_n) + \\
& (a_2b_2)(a_1b_1) + (a_2b_2)(a_2b_2) + \cdots + (a_2b_2)(a_nb_n) + \\
& \hspace{1in} \vdots\\
& (a_nb_n)(a_1b_1) + (a_nb_n)(a_2b_2) + \cdots + (a_nb_n)(a_nb_n)\\\\
= & (a_1a_1)(b_1b_1) + (a_1a_2)(b_1b_2) + \cdots + (a_1a_n)(b_1b_n) + \\
& (a_2a_1)(b_2b_1) + (a_2a_2)(b_2b_2) + \cdots + (a_2a_n)(b_2b_n) + \\
& \hspace{1in} \vdots\\
& (a_na_1)(b_nb_1) + (a_na_2)(b_nb_2) + \cdots + (a_na_n)(b_nb_n)
\end{align*}

Observe that the expanded sum is equivalent to the dot product of the vectors containing all pair-wise interaction terms. So, $\phi_{Power(2)}(\boldsymbol{x}) = (x_1x_1, x_1x_2, \cdots, x_1x_n, x_2x_1, x_2x_2, \cdots, x_2x_n, \cdots, x_nx_1, x_nx_2, \cdots, x_nx_n)^T$ in this case and $K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = \phi_{Power(2)}(\boldsymbol{a}) \cdot \phi_{Power(2)}(\boldsymbol{b}) = (\boldsymbol{a} \cdot \boldsymbol{b})^2$. However, notice we don't have to compute $\phi_{Power(2)}$ if we take $K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = (\boldsymbol{a} \cdot \boldsymbol{b})^2$ instead. Applying this kernel on our data clearly results in a better classification, although not quite perfect:

```{python, echo=F, cache=T }
plot_with_boundary(power_kernel(2),"Power(2)")
```

Likewise, one can show that:\begin{align*}
K_{Power(3)}(\boldsymbol{a}, \boldsymbol{b}) = & (\boldsymbol{a} \cdot \boldsymbol{b})^3 \text{ corresponds to the transformation containing all 3-way interaction terms}\\
K_{Power(4)}(\boldsymbol{a}, \boldsymbol{b}) = & (\boldsymbol{a} \cdot \boldsymbol{b})^4 \text{ corresponds to the transformation containing all 4-way interaction terms}\\
&\hspace{1in}\vdots\\
K_{Power(n)}(\boldsymbol{a}, \boldsymbol{b}) = & (\boldsymbol{a} \cdot \boldsymbol{b})^n \text{ corresponds to the transformation containing all n-way interaction terms}
\end{align*}

Let's try out several Power Kernels:

```{python, echo=F, cache=T }
for i in range(3,6+1):
  plot_with_boundary(power_kernel(i),f"Power({i})")
```

Like the base Kernel, the odd-power Kernels suck at classifying the data, while even power kernels do a fantastic job, specially after 4. This oddity can be mitigated by a principle learned in Applied Statistics - whenever we include high order terms, we also want to include all lower level terms. So, let's inspect the following kernel:

$K_{Poly(2)}(\boldsymbol{a}, \boldsymbol{b}) = (\boldsymbol{a} \cdot \boldsymbol{b} + 1)^2 = \underset{\text{2-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^2} + \underset{\text{1-way}}{2(\boldsymbol{a} \cdot \boldsymbol{b})} + \underset{\text{0-way}}{1}$ 

So, $K_{Poly(2)}$ gives us the 2-way interaction terms and all the lower order terms. Likewise,  $K_{Poly(n)}= (\boldsymbol{a} \cdot \boldsymbol{b} + 1)^n$ gives us the n-way interaction terms and below, precisely what we wanted. This is also the polynomial kernel for SVM with $\gamma$ = 1. After trying $K_{Poly(2)}$ on our data, we get a similar plot to the one produced by $K_{Power(2)}$:

```{python, echo=F, cache=T }
plot_with_boundary(poly_kernel(2),"Poly(2)")
```

And when we run it with higher degree Kernels, we can observe that the even-degree Kernels look very similar. However, the odd degrees Kernels can in fact classify the data efficiently as they also "include" even-degree terms.

```{python, echo=F, cache=T }
for i in range(3,6+1):
  plot_with_boundary(poly_kernel(i),f"Poly({i})")
```

```{python, echo=F, cache=T }
plot_with_boundary(power_kernel(50),"Power(50)")
plot_with_boundary(poly_kernel(50),"Poly(50)")
```

It's also important to note that weird boundaries arise when our Kernel degree gets higher. $K_{Power(50)}$ is similar to $K_{Power(2)}$, but inside-out. This proposes an interesting conundrum: which n should we pick? We could try using using cross-validation. However, our friend ***Taylor*** might have a way of trying out all n values at the same time, while giving more weight to lower order terms than the higher order terms (following the principle of parsimony).

Recall: $\operatorname{exp}(x) = 1 + \dfrac{1}{1!}x^1 + \dfrac{1}{2!}x^2 + \dfrac{1}{3!}x^3 + \dfrac{1}{4!}x^4 + \cdots$


Then by plugging $\boldsymbol{a} \cdot \boldsymbol{b}$ for $x$, we get: $\operatorname{exp}(\boldsymbol{a} \cdot \boldsymbol{b}) = \underset{\text{0-way}}{1} + 
\dfrac{1}{1!}\underset{\text{1-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^1} + 
\dfrac{1}{2!}\underset{\text{2-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^2} + 
\dfrac{1}{3!}\underset{\text{3-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^3} + 
\dfrac{1}{4!}\underset{\text{4-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^4} + \cdots$

Which gives us ALL n-way interaction terms, weighing the lower terms more and the higher terms less. This also means we're essentially computing the dot product of an infinite-dimensional transformation, without having to compute infinite transformations. However, to get to the Radial Basis Function, we need a couple transformations:

In practice, any SVM algorithm normalizes the data before performing the classification. So, let's multiply our kernel by $\operatorname{exp}\left(-\dfrac{\Vert\boldsymbol{a}\Vert^2_2 + \Vert\boldsymbol{b}\Vert^2_2}{2}\right)$. That way, outliers or observations that deviate a lot contribute less to our model.\begin{align*}
\operatorname{exp}\Big(\boldsymbol{a} \cdot \boldsymbol{b}\Big)\operatorname{exp}\left(-\dfrac{\Vert\boldsymbol{a}\Vert^2_2 + \Vert\boldsymbol{b}\Vert^2_2}{2}\right) &= 
\operatorname{exp}\left(\boldsymbol{a} \cdot \boldsymbol{b}-\dfrac{\Vert\boldsymbol{a}\Vert^2_2 + \Vert\boldsymbol{b}\Vert^2_2}{2}\right)\\
&= \operatorname{exp}\left(-\dfrac{\Vert\boldsymbol{a}\Vert^2_2 + \Vert\boldsymbol{b}\Vert^2_2- 2 \boldsymbol{a} \cdot \boldsymbol{b}}{2}\right)\\
&= \operatorname{exp}\left(-\dfrac{1}{2}\Vert\boldsymbol{a}-\boldsymbol{b}\Vert^2_2\right)
\end{align*}


Now, let's raise it to the $2\gamma$ to add a control parameter. The control parameter adjusts for the importance of higher order terms. The higher it is, the more relevant they become.

$\left(\operatorname{exp}\left(-\dfrac{1}{2}\Vert\boldsymbol{a}-\boldsymbol{b}\Vert^2_2\right)\right)^{2\gamma} = \operatorname{exp}\Big(-\gamma \Vert\boldsymbol{a}-\boldsymbol{b}\Vert^2_2\Big)$


Finally, we have arrived to the Radial Basis Function Kernel:

$K_{RBF}(\boldsymbol{a}, \boldsymbol{b}) = \operatorname{exp}\Big(-\gamma \Vert\boldsymbol{a}-\boldsymbol{b}\Vert^2_2\Big)$

\newpage

After running the SVM with the RBF kernel, we get pretty nice non-linear decision boundaries. Observe that the "majority class" becomes the "background", while the "minority class" creates some "splodges" across the plane.

```{python, echo=F, cache=T }
plot_with_boundary("rbf")
```

In essence RBF is so special because it performs the optimization over an infinite-dimensional feature space. All that with a really simple formula, which allows for very intricate decision boundaries with minimal computational power.

### Classifying the data

Now we know all about fitting an SVM model with a custom Kernel. However, once the model has been fit, how do we classify our data? We compute the kernel between $\boldsymbol{\beta}$ and an observation, subtract c, and take the sign:\begin{align*}
&\text{if } K(\boldsymbol{\beta}, \xb_i) - c > 0 \rightarrow y_i = 1\\
&\text{if } K(\boldsymbol{\beta}, \xb_i) - c < 0 \rightarrow y_i = -1
\end{align*}


## Radial Basis Function Considerations

### Assumptions for Custom Kernel SVM

1.- There are two classes to classify

2.- $K$, the matrix composed of all pairwise kernels ($k_ij = K( \xb_i , \xb_j )$), is positive semi-definite. This is satisfied by the Radial Basis Kernel.

### Model Parameters
The RBF model has two tunne-able parameters: cost and gamma. 

1.- The cost parameter ($\xi$) is the cost of missclassifying a data point (Soft Margin case only). A higher $\xi$ will lead to a more complex model that will try to classify all data points correctly. It trades off missclassification of training examples against increasing the margin. Larger values of cost will lead to a smaller margin. In this way $\xi$ behaves as a regularization parameter. 

2.- The gamma ($\gamma$) parameter controls how far the influence of a single training example reaches. A low $\gamma$ will consider points far away from the decision boundary in calculations, while a high $\gamma$ will consider only points close to the decision boundary. The $\gamma$ parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.


### Strengths and Weaknesses
Strengths:

1. RBFs are able to perform optimization over an $\infty$-dimensional space using a relatively simple formula, making them computationally inexpensive while still having a lot of power. 

2. There are no preassumptions that need to be made about the data, making it a great tool when the distribution of data is not known. 

3. RBF decisions are local ones, making it a more robust algorithm that is not sensitive to outliers, in comparison to some other kernels. 

4. The hyperparameter $\gamma$ and the cost parameter (soft margin case) allow us to be more flexible about model specificity. 

Weaknesses:

1. RBFs are non probablistic, meaning points are only classified as being in one class or the other. We cannot adjust the probability cutoff for classifying points like we might in logistic regression. However, we *can* FORCE a probabilistic model by fitting a logistic regression around the RBF model.

2. Like SVMs, we do not get easily interpretable coefficients. So, it becomes difficult to assess the influence of a specific variable on classification.

3. While this isn't necessarily a weakness per se, it's worth mentioning that the RBF kernel is non-parametric. This makes traditional types of inference significantly harder. 

4. Because RBFs are more complex models, it is easier to overfit them. 

## Example Spiral Data

```{r, echo=FALSE, cache=TRUE, out.width="70%"}
n <- 1000
r <- 6 * pi + 1
r_offset <- 2
colors <- c("blue", "red")

generate_spiral <- function(n, r, r_offset, colors, seed) {
  set.seed(seed)

  # Generate random values for r and theta
  r <- runif(n, 1, 6 * pi + 1)
  theta <- runif(n, 0, 2 * pi)

  # Classify observations based on r and theta
  class <- ifelse((r + theta) %% (2 * pi) < pi, 1, 0)

  # Create a data frame with the data
  data <- data.frame(r, theta, class)

  # Create colors based on class
  data$color <- ifelse(data$class == 1, colors[2], colors[1])

  # Convert polar coordinates to Cartesian coordinates
  data$x <- data$r * cos(data$theta)
  data$y <- data$r * sin(data$theta)

  # Offset the data
  for (j in 1:n) {
    r_offset_val <- runif(1, 0, r_offset)
    theta_offset_val <- runif(1, 0, 2 * pi)

    # Convert to Cartesian coordinates
    data$x[j] <- data$x[j] + r_offset_val * cos(theta_offset_val)
    data$y[j] <- data$y[j] + r_offset_val * sin(theta_offset_val)

    # Convert back to polar coordinates
    data$r[j] <- sqrt(data$x[j]^2 + data$y[j]^2)
    data$theta[j] <- atan2(data$y[j], data$x[j])
  }

  return(data)
}

data <- generate_spiral(n, r, r_offset, colors, seed = 2024)
```

To show the power of the Radial Basis Function we will first use a generated data set: it matches a spiral pattern with a bit of overlap (noise) between the two classes. The data was generated with polar coordinates, so we might think about converting to polar coordinates first. However, if this data weren't generated we might not make that connection. This is where the RBF kernel can be very useful.

```{r, echo=FALSE, out.width="70%"}
# Plot the data in the x and y coordinates
plot(data$x, data$y, col = data$color, pch = 19, xlab = "x", ylab = "y")
```

### Fitting the Model

Now lets use the RBF kernel to classify this data. First, let's use a cost of 1 and a gamma of 1 (the default values).

```{r, message=FALSE, warning=FALSE}
# Load the required svm library
library(e1071)
library(caret)

# Convert class to a factor
data$class <- as.factor(data$class)

# Create a data frame with only the class and the x and y coordinates
data2 <- data.frame(class = data$class, x = data$x, y = data$y)
data2$class <- as.factor(data2$class)

# Use a radial basis function kernel to classify the data with the SVM function
(svmfit <- svm(class ~ ., data = data2, kernel = "radial"))
```

### Diagnostics

#### Confusion Matrix
As RBF (and SVM) are classification algorithms, the confusion matrix diagnostics serve as a well tested diagnostic toolkit. Below we have the confusion matrix for our RBF model. The accuracy is 55.1%, which is not much better than random guessing. Note: The data in this project has a split close enough to 50-50. This means that accuracy works fine as an overall metric to gauge the models ability. For all future confusion matrices we will look at just accuracy and kappa.

```{r, cache=TRUE}
# Create a confusion matrix to evaluate the SVM model
confusionMatrix(predict(svmfit, data2), data2$class)
```

#### Decision Boundary
Another way of assessing the model is to look at the decision boundary. The following code plots the decision boundary for our model (both with and without the points):

```{r, fig.show="hold", out.width="50%", cache=TRUE}
# Define colors for data points and decision boundary
point_colors <- c("blue", "red")
boundary_colors <- c("skyblue", "orange")

# Plot the decision boundary
x1_grid <- seq(min(data2$x), max(data2$x), length.out = 100)
x2_grid <- seq(min(data2$y), max(data2$y), length.out = 100)
grid <- expand.grid(x = x1_grid, y = x2_grid)

predicted_labels <- predict(svmfit, newdata = grid)

# Plot the decision boundary with the data points
# Plot the decision boundary
plot(grid$x, grid$y, col = boundary_colors[predicted_labels],
     pch = ".", cex = 3.5, xlab = "x", ylab = "y")

# Plot the data points
points(data2$x, data2$y, col = point_colors[data2$class], pch = 19)
legend("topright", legend = levels(data2$class), col = point_colors, pch = 19)

# Plot the decision boundary with the data points
plot(grid$x, grid$y, col = boundary_colors[predicted_labels], pch = ".",
     cex = 3.5, xlab = "x", ylab = "y")
legend("topright", legend = levels(data2$class), col = point_colors, pch = 19)
```

We can observe that the decision boundary doesn't capture the spiraly-ness of the data very well. This is in part since we used the default $\xi$ and $\gamma$.

### Cross Validating RBF
We can use cross-validation to find the best values for $\xi$ and $\gamma$. Fortunately, the `svm` function has built in cross-validation. Setting the `cross` parameter to the number of folds, in this case 5. However, the `svm` function does not *find* the best set of parameters, it only calculates statistics for a given set of parameters. The following function performs the full cross-validation for a RBF model:
```{r, echo=TRUE, cache=TRUE}
# First write a simple cross validation function
cross_validate <- function(folds, costs, gammas, data, seed) {
  # Create a data frame to store the results
  results <- data.frame(cost = numeric(0), gamma = numeric(0),
                        accuracy = numeric(0))

  # Loop through each cost and gamma value
  for (cost in costs) {
    for (gamma in gammas) {
      # Set seed so the folds should be the same each time
      set.seed(seed)

      # Use cross-validation to find the best cost and gamma values
      svm_cross <- svm(class ~ ., data = data, kernel = "radial",
                       cross = folds, cost = cost, gamma = gamma)

      # Store the results
      results <- rbind(results, data.frame(cost = cost, gamma = gamma,
                                           accuracy = svm_cross$tot.accuracy))
    }
  }

  return(results)
}
```

```{r, echo=FALSE, cache=TRUE}
# Create a simpler confusion matrix function
simpler_cm <- function(cm.out) {
    print(cm.out[[2]])
    print(cm.out[[3]][1:2])
}
```


After running our function and filtering for the best 5 results, we get the following table. We can observe that a cost of 1 and a gamma of 10 are the best values for this data with an average accuracy of 74.3% on the test folds. 
```{r, cache=TRUE}
# Run our function
validation_data_frame <- cross_validate(5, c(0.1, 1, 10, 100, 1000),
                                        c(0.01, 0.1, 1, 10, 100),
                                        data2, seed = 2024)

# Print the top 5 best cost and gamma values
print(validation_data_frame[order(-validation_data_frame$accuracy), ][1:5, ])
```

Now let's use these values to fit a new model. By analyzing the Confusion Matrix, we can observe that the accuracy rose all the way to 82.1% on the training set. This is a huge improvement!!!

\newpage

```{r, echo=FALSE, cache=TRUE}
# Use the best cost and gamma values to classify the data
svmfit_best <- svm(class ~ ., data = data2, kernel = "radial", cost = 1,
                   gamma = 10)

# Create a confusion matrix to evaluate the SVM model
confusionMatrix(predict(svmfit_best, data2), data2$class) |> simpler_cm()
```

The decision boundary also looks way better. It isn't quite perfect, Once again the majority class becomes the background and the minority class generates weird patches. Nonetheless, the fitted decision boundary is much closer to the true decision boundary of the data than before. 

```{r, echo=FALSE, fig.show="hold", out.width="50%", cache=TRUE}
predicted_labels_best <- predict(svmfit_best, newdata = grid)

plot(grid$x, grid$y, col = boundary_colors[predicted_labels_best], pch = ".",
     cex = 3.5, xlab = "x", ylab = "y")

# Plot the data points
points(data2$x, data2$y, col = point_colors[data2$class], pch = 19)
legend("topright", legend = levels(data2$class), col = point_colors, pch = 19)

# Plot the decision boundary
plot(grid$x, grid$y, col = boundary_colors[predicted_labels_best], pch = ".",
     cex = 3.5, xlab = "x", ylab = "y")
legend("topright", legend = levels(data2$class), col = boundary_colors,
       pch = 19)
```

### Testing Dataset

The final step is to see how this model performs on some test data generated the same way. By observing the Test Confusion Matrix, we get a test accuracy of 75.6% on the test data. This is a lower than the 82.1% training accuracy. However, the test accuracy is higher than the best average cross-validated accuracy (74.3%). This shows that the model is generalizing well to new data.

```{r, echo=FALSE}
# Create new test data
n <- 1000
r <- 6 * pi + 1
r_offset <- 2
colors <- c("blue", "red")
test_data <- generate_spiral(n, r, r_offset, colors, seed = 2025)

# Convert class to a factor
test_data$class <- as.factor(test_data$class)

# Create a data frame with only the class and the x and y coordinates
test_data2 <- data.frame(class = test_data$class, x = test_data$x,
                         y = test_data$y)
test_data2$class <- as.factor(test_data2$class)

# Create a confusion matrix to evaluate the SVM model
confusionMatrix(predict(svmfit_best, test_data2), test_data2$class) |>
  simpler_cm()
```

### Best Case Scenario
Since this is generated data, we can also classify the data using the true decision boundary (spiral - noise) and compare the results to the ones obtained by our model. From the Confusion Matrix, we can observe that the added noise causes the best possible accuracy to be 80.7%. This implies our model is only 5.2% below the accuracy of the best model we *could* get.

```{r, echo = F, , cache=TRUE}
post_offset_class <- numeric(n)
new_r <- numeric(n)
new_theta <- numeric(n)

# This code is how I originally classified the data before offsetting it
for (j in 1:n) {
  new_r[j] <- sqrt(test_data2$x[j]^2 + test_data2$y[j]^2)
  new_theta[j] <- atan2(test_data2$y[j], test_data2$x[j])

  ifelse(new_theta[j] < 0, new_theta[j] <- new_theta[j] + 2 * pi,
         new_theta[j] <- new_theta[j])

  # Classify observations based on r and theta
  post_offset_class[j] <- ifelse((new_r[j] + new_theta[j]) %% (2 * pi) < pi,
                                 1, 0)
}
```

```{r, echo=FALSE, , cache=TRUE}
# Add the results to the data frame
test_data2$post_offset_class <- post_offset_class
test_data2$new_r <- new_r
test_data2$new_theta <- new_theta

# Make the post offset class a factor
test_data2$post_offset_class <- as.factor(test_data2$post_offset_class)

# Now compare the true class to the predicted class given by the true decision boundary
confusionMatrix(test_data2$post_offset_class, test_data2$class) |> simpler_cm()
```

We can also overlay the plot of the fit decision boundary (lightblue-orange) and a plot for the true decision boundary (light-dark). By doing so, we can observe that the fit decision boundary estimates the true decision boundary surprisingly well.

```{r, echo=FALSE, cache=TRUE, out.width="70%"}
# Create a grid of points for prediction
x1_grid <- seq(min(test_data2$x), max(test_data2$x), length.out = 100)
x2_grid <- seq(min(test_data2$y), max(test_data2$y), length.out = 100)

grid <- expand.grid(x = x1_grid, y = x2_grid)

# Make the grid a data frame of x and y coordinates
grid_data <- data.frame(x = grid$x, y = grid$y)

# Classify the grid data
for (j in 1:nrow(grid_data)) {
  new_r <- sqrt(grid_data$x[j]^2 + grid_data$y[j]^2)
  new_theta <- atan2(grid_data$y[j], grid_data$x[j])

  ifelse(new_theta < 0, new_theta <- new_theta + 2 * pi, new_theta <- new_theta)

  # Classify observations based on r and theta
  grid_data$class[j] <- ifelse((new_r + new_theta) %% (2 * pi) < pi, 1, 0)
}

# Plot the true decision boundary
plot(grid_data$x, grid_data$y, col = grid_data$class, pch = ".", cex = 3.5,
     xlab = "x", ylab = "y")

# Plot the decision boundary found by the model
predicted_labels_best <- predict(svmfit_best, newdata = grid)

points(grid$x, grid$y, col = boundary_colors[predicted_labels_best], pch = "x",
       cex = .85)
```

### Facing Other Models

Now that we have seen the power of the RBF kernel we can compare it to other models. We will compare it to a linear kernel SVM, a polynomial kernel SVM, a logistic regression model without many interactions. We will also compare it to a KNN model. 

Lets start with a linear kernel SVM and a polynomial kernel SVM since those are part of the same family. After cross-validation (for the polynomial kernel the best degree, cost, and gamma are 5, 10 and 0.1) and testing, we obtained each models' confusion Matrix to realize they're marginally better than just randomly guessing. From the confusion matrix, we can observe that the linear kernel model is  basically guessing. This is in part since no clear line or simple curve splits the data.


<!-- ## Cross validation for polynomial Kernel-->
<!-- ## Showing and running this code takes up too much time and space on the document -->
<!-- ## Not including this allows for a more seamless side-by-side polynomial and linear models -->
```{r, echo=FALSE, cache=TRUE, eval=F}
# Use a polynomial kernel to classify the data with the SVM function

# Write a cross validation function for polynomial kernels that takes degrees, cost, and gamma into account
cross_poly <- function(folds, degrees, costs, gammas, dataset, seed) {
  results <- data.frame(degree = numeric(0), cost = numeric(0),
                        gamma = numeric(0), accuracy = numeric(0))
  iteration <- 0

  for (degree in degrees) {
    for (cost in costs) {
      for (gamma in gammas) {
        set.seed(seed)
        print(iteration)
        iteration <- iteration + 1
        svm_cross <- svm(class ~ ., data = dataset, kernel = "polynomial",
                         degree = degree, cross = folds, cost = cost,
                         gamma = gamma)

        results <- rbind(results, data.frame(degree = degree, cost = cost,
                                             gamma = gamma,
                                             accuracy = svm_cross$tot.accuracy))
      }
    }
  }

  return(results)
}

# Run our function
degrees <- c(4, 5, 6)
costs <- c(1, 10, 100)
gammas <- c(0.1, 1, 10)

# Running this code will take awhile, maybe uncomment it when knitting the pdf for the final time.
validation_data_frame_poly <- cross_poly(2, degrees, costs, gammas, data2, seed = 2024)

# Print the top 5 best degree, cost, and gamma values
print(validation_data_frame_poly[order(-validation_data_frame_poly$accuracy), ][1:5, ])
```



```{r, cache=TRUE, echo=FALSE}
# Use a linear kernel to classify the data with the SVM function
svmfit_linear <- svm(class ~ ., data = data2, kernel = "linear", cost = 1000)

# Create a confusion matrix to evaluate the SVM model
noquote("Linear SVM:")
confusionMatrix(predict(svmfit_linear, data2), data2$class) |> simpler_cm()

# Make the best poly model based on the cross validation
svm_poly <- svm(class ~ ., data = data2, kernel = "polynomial", degree = 5,
                cost = 10, gamma = 0.1)

# Check the svm_poly model on a confusion matrix
noquote("Polynomial SVM:")
confusionMatrix(predict(svm_poly, data2), data2$class) |> simpler_cm()
```

Once again, the plots show that no single line or simple curve efficiently separates the data.

```{r, echo=FALSE, cache=TRUE, fig.show="hold", out.width="50%"}
predicted_labels_linear <- predict(svmfit_linear, newdata = grid)

plot(grid$x, grid$y, col = boundary_colors[predicted_labels_linear], pch = ".",
     cex = 3.5, xlab = "x", ylab = "y")

# Plot the data points
points(data2$x, data2$y, col = point_colors[data2$class], pch = 19)

predicted_labels_poly <- predict(svm_poly, newdata = grid)

plot(grid$x, grid$y, col = boundary_colors[predicted_labels_poly], pch = ".",
     cex = 3.5, xlab = "x", ylab = "y")

points(data2$x, data2$y, col = point_colors[data2$class], pch = 19)
```

Now let's check how a Logistic Regression model does on this data. After fitting the model, we get the following confusion Matrix. The Logistic Regression has an accuracy of 54.4%, which is better than the previous 2 SVM models, however, it is still not much better than random guessing.

```{r, echo=FALSE}
# Use a logistic regression model to classify the data (add many interaction terms)
logit_model <- glm(class ~ ., data = data2, family = "binomial")

#summary(logit_model)

# Predict using the model
predicted_probabilities <- predict(logit_model, type = "response")

# Change all the Falses to 0 and Trues to 1
predicted_probabilities2 <- ifelse(predicted_probabilities > 0.5, 1, 0)

# Create a confusion matrix to evaluate the model
confusionMatrix(as.factor(predicted_probabilities2), as.factor(data2$class)) |>
  simpler_cm()
```
If we peek at the predicted probabilities, we can observe the model is not very confident about any decisions.
```{r, echo=FALSE}
# peek at probabilities
print(predicted_probabilities |> head())
```

Finally, the plot looks like the linear kernel since we do not have any interaction terms. The decision boundary is a straight line that doesn't capture the spiral shape of the data. Adding interaction terms actually made model accuracy worse.

```{r, echo=FALSE, out.width="70%"}
# Create grid data for the decision boundary
x1_grid <- seq(min(data2$x), max(data2$x), length.out = 100)
x2_grid <- seq(min(data2$y), max(data2$y), length.out = 100)
grid <- expand.grid(x = x1_grid, y = x2_grid)

# Predict using the model
grid$predicted_probabilities <- predict(logit_model, newdata = grid, type = "response")

grid$predictions <- ifelse(grid$predicted_probabilities > 0.5, 1, 0)

# Plot the decision boundary
plot(grid$x, grid$y, col = boundary_colors[as.factor(grid$predictions)], pch = ".",
     cex = 3.5, xlab = "x", ylab = "y")
points(data2$x, data2$y, col = point_colors[data2$class], pch = 19)
```

Finally let's check how a KNN model does on this data. We find that the best k value is 3. We can expect the KNN model to perform much better for weird decision boundaries, so let's calculate both confusion matrices: training and testing. We can observe that the training accuracy of 85.7% and a testing accuracy of 70.4%. While the training accuracy is greater when compared to RBF (82.1%), the training accuracy is in fact lower greater when compared to RBF (75.6%). This is an indicator that the KNN model is significantly overfit and performs worse when predicting new data than RBF in this scenario.

```{r, echo=FALSE, results='hide'}
# Load the required class library
library(class)

# Write a cross validation function for KNN that takes k into account
cross_knn <- function(folds, ks, dataset, seed) {
  set.seed(seed)
  knn_model_cv <- train(class ~ .,
    data = dataset, method = "knn",
    trControl = trainControl(method = "cv", number = folds),
    tuneGrid = data.frame(k = ks)
  )
  return(knn_model_cv)
}

# Run our function
ks <- seq(1, 300, 2)

# Run the cross validation function
validation_data_frame_knn <- cross_knn(5, ks, data2, seed = 2024)

# Print the top 5 best k values
print(validation_data_frame_knn$results[
  order(-validation_data_frame_knn$results$Accuracy), ][1:5, ])
```

```{r, echo=FALSE}
# Use the best k value found from cross validation
k <- 3

# Use a KNN model to classify the data
knn_model <- knn3(class ~ ., data = data2, k = k)
```

```{r, echo=FALSE}
# Create a confusion matrix to evaluate the KNN model
predicted_labels_knn <- predict(knn_model, newdata = data2, type = "class")
noquote("Training KNN Confusion Matrix:")
confusionMatrix(predicted_labels_knn, data2$class) |> simpler_cm()

# Create a confusion matrix to evaluate the KNN model
knn_model_test <- predict(knn_model, newdata = test_data2, type = "class")
noquote("Testing KNN Confusion Matrix:")
confusionMatrix(knn_model_test, test_data2$class) |> simpler_cm()
```

While the KNN model does capture the overall spiral shape, it's not as consistant as the RBF model.
```{r, echo=FALSE, out.width="70%"}
# Plot the decision boundary
predicted_labels_knn <- predict(knn_model, newdata = grid, type = "class")

plot(grid$x, grid$y, col = boundary_colors[predicted_labels_knn], pch = ".",
     cex = 3.5, xlab = "x", ylab = "y")
points(data2$x, data2$y, col = point_colors[data2$class], pch = 19)
```


In conclusion, the RBF model was one of the only two models capable of capturing such intricate decision boundaries. And when compared to the other alternative (KNN), RBF was far superior in this scenario.

## Real Data Set

Now that we have demonstrated the power of the RBF kernel on a generated data set, we will now demonstrate the power of the RBF kernel on a real data set. We will use the famous MNIST data set which contains images of handwritten digits. We will only use the 2's and 7's from the data set since they are the most similar and thus the most difficult to classify. We will use the same process as before to classify the data and plot the decision boundary. This data set is also nice since it is a good example of where an RBF kernel would be useful since the decision boundary is not linear, and can also be graphed in 2 dimensions. 

For this dataset `x_1` is the proportion of dark pixels in the upper-left quadrant, `x_2` is the proportion of dark pixels in the lower-right quadrant, and `y` is the true classification for the digit (2 or 7). By plotting the data, we can observe that a single curve should be able to separate the data. More specifically, a parabola facing to the left should enclose the 2's good enough.

```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width="70%"}
# Load the required libraries
library(MASS)
library(caret)
library(e1071)
library(dslabs)
library(tidyverse)

# Load the MNIST data set
data("mnist_27")

# Plot with base R
plot(mnist_27$train$x_1, mnist_27$train$x_2,
     col = point_colors[mnist_27$train$y],
     pch = 19, xlab = "Proportion of dark pixels in the upper-left quadrant",
     ylab = "Proportion of dark pixels in the lower-right quadrant")

# Add a legend
legend("topright", legend = levels(as.factor(mnist_27$train$y)),
       col = point_colors, pch = 19)
```

First let's try fitting a linear kernel to the data to get a baseline. From here, all the code will use the training data until we return to see how it does on the test data. From the Confusion Matrix we can observe that our linear kernel does not quite do the job. It's accuracy of 78.88% is not as good as we would like, but it gives us a good baseline to compare the other models to. Now let's try an untunned RBF kernel.

```{r, echo=FALSE, out.width="70%"}
# Fit a linear kernel to the data
svmfit_linear_mnist <- svm(y ~ ., data = mnist_27$train, kernel = "linear",
                           cost = 1000)

# Create a confusion matrix to evaluate the SVM model
confusionMatrix(predict(svmfit_linear_mnist, mnist_27$train), 
                mnist_27$train$y) |> simpler_cm()

# Create a grid of points for prediction
x1_grid <- seq(min(mnist_27$train$x_1), max(mnist_27$train$x_1),
               length.out = 100)
x2_grid <- seq(min(mnist_27$train$x_2), max(mnist_27$train$x_2),
               length.out = 100)
grid <- expand.grid(x_1 = x1_grid, x_2 = x2_grid)

predicted_labels_linear_mnist <- predict(svmfit_linear_mnist, newdata = grid)

# Plot the decision boundary
plot(grid$x_1, grid$x_2, col = boundary_colors[predicted_labels_linear_mnist],
     pch = ".", cex = 3.5,
     xlab = "Proportion of dark pixels in the upper-left quadrant",
     ylab = "Proportion of dark pixels in the lower-right quadrant")
points(mnist_27$train$x_1, mnist_27$train$x_2,
       col = point_colors[mnist_27$train$y], pch = 19)
```

Even with the default values of cost and gamma the RBF kernel seems to do a better job of classifying the data than the linear kernel. The accuracy of 83.38% is better than the linear kernel. The decision boundary also looks better than the linear kernel, seeming to follow the data better. Now let's try to find the best values for cost and gamma using cross-validation.
\newpage

```{r, echo=FALSE, out.width="70%"}
# Use a radial basis function kernel to classify the data with the SVM function
svmfit_rbf_mnist <- svm(y ~ ., data = mnist_27$train, kernel = "radial",
                        cost = 1, gamma = 1)

# Create a confusion matrix to evaluate the SVM model
confusionMatrix(predict(svmfit_rbf_mnist, mnist_27$train), mnist_27$train$y) |>
  simpler_cm()

predicted_labels_rbf_mnist <- predict(svmfit_rbf_mnist, newdata = grid)

# Plot the decision boundary
plot(grid$x_1, grid$x_2, col = boundary_colors[predicted_labels_rbf_mnist],
     pch = ".", cex = 3.5,
     xlab = "Proportion of dark pixels in the upper-left quadrant",
     ylab = "Proportion of dark pixels in the lower-right quadrant")
points(mnist_27$train$x_1, mnist_27$train$x_2,
       col = point_colors[mnist_27$train$y], pch = 19)
```



```{r, echo=FALSE, out.width="70%",, results='hide'}
# Convert to a matrix with the proper column names
training_matrix <- as.data.frame(mnist_27$train)
colnames(training_matrix) <- c("class", "x_1", "x_2")
training_matrix$class <- as.factor(training_matrix$class)

# Run our cross validation function
validation_data_frame_mnist <- cross_validate(10, c(0.1, 1, 10, 100, 1000),
                                              c(0.01, 0.1, 1, 10, 100),
                                              training_matrix, seed = 2024)

# Print the top 5 best cost and gamma values
print(validation_data_frame_mnist[order(-validation_data_frame_mnist$accuracy),
                                  ][1:5, ])
```

After 10-fold cross-validation, we get that a cost of 0.1 and a gamma of 1 seems to do the best at predicting new unseen data getting an accuracy of 83.375% on the training dataset. From the confusion matrix we observe an accuracy of 83.88%, a marginal improvement by 0.5%. It seems to only reclassify the bottom left corner of the data from 2's to 7's and slightly pull in the amount of area classified as 2's along the edge causing more points to be classified as 7's overall. 

```{r, echo=FALSE, out.width="70%"}
# Use the best cost and gamma values to classify the data
svmfit_best_mnist <- svm(y ~ ., data = mnist_27$train, kernel = "radial",
                         cost = 0.1, gamma = 1)

# Create a confusion matrix to evaluate the SVM model
confusionMatrix(predict(svmfit_best_mnist, mnist_27$train), mnist_27$train$y) |>
  simpler_cm()

predicted_labels_best_mnist <- predict(svmfit_best_mnist, newdata = grid)

# Plot the decision boundary
plot(grid$x_1, grid$x_2, col = boundary_colors[predicted_labels_best_mnist],
     pch = ".", cex = 3.5,
     xlab = "Proportion of dark pixels in the upper-left quadrant",
     ylab = "Proportion of dark pixels in the lower-right quadrant")
points(mnist_27$train$x_1, mnist_27$train$x_2,
       col = point_colors[mnist_27$train$y], pch = 19)
```

From the example data set we know that k-Nearest Neighbors also does well on these types of data sets so let's see how it does on this data set to make comparisons and see if we have found the best model.

```{r, echo=FALSE, cache=FALSE, results='hide'}
# Create a list of k values to try
ks <- seq(1, 300, by = 2)

# Create a matrix with the proper column names
training_matrix <- as.data.frame(mnist_27$train)
colnames(training_matrix) <- c("class", "x", "y")

# Run the cross validation function
validation_data_frame_knn_mnist <- cross_knn(10, ks, training_matrix,
                                             seed = 2024)

# Print the top 5 best k values
print(validation_data_frame_knn_mnist$
        results[order
                (-validation_data_frame_knn_mnist$results$Accuracy), ][1:5, ])
```

From the cross validation we get that a k value of 83 is the best value for this data set with an accuracy of 83.38% on the training dataset. After training such model, we observe the decision boundary looks similar to the one the RBF kernel produced with the default values of cost and gamma, and the accuracy is the exact same at 83.38%. It seems that this decision boundary just slightly predicts more 2's than the RBF kernel, but overall they are very similar.

```{r, echo=FALSE, out.width="70%"}
# Use the best k value found from cross validation
k <- 83

# Use a KNN model to classify the data
knn_model_mnist <- knn3(class ~ ., data = training_matrix, k = k)

# Create a confusion matrix to evaluate the KNN model
predicted_labels_knn_mnist <- predict(knn_model_mnist,
                                      newdata = training_matrix,
                                      type = "class")
confusionMatrix(predicted_labels_knn_mnist, training_matrix$class) |>
  simpler_cm()

# Make new grid
x1_grid <- seq(min(training_matrix$x), max(training_matrix$x), length.out = 100)
x2_grid <- seq(min(training_matrix$y), max(training_matrix$y), length.out = 100)
grid <- expand.grid(x = x1_grid, y = x2_grid)

# Plot the decision boundary
predicted_labels_knn_mnist <- predict(knn_model_mnist, newdata = grid,
                                      type = "class")

plot(grid$x, grid$y, col = boundary_colors[predicted_labels_knn_mnist],
     pch = ".", cex = 3.5,
     xlab = "Proportion of dark pixels in the upper-left quadrant",
     ylab = "Proportion of dark pixels in the lower-right quadrant")
points(mnist_27$train$x_1, mnist_27$train$x_2,
       col = point_colors[mnist_27$train$y], pch = 19)
```

\newpage
Finally, lets compare the peformance of our two best models on the test dataset. The RBF testing accuracy is 83.5% (basically the same as the training). This means we have found a good model that generalizes well to new data and also did well on it's own training data. Meanwhile, the KNN model got a testing accuracy of 82%. This is also a good testing accuracy. However, the RBF model is clearly less overfit than the KNN model.
```{r, echo=FALSE}
# Create a confusion matrix to evaluate the SVM model
noquote("Testing Dataset RBF Confusion Matrix:")
confusionMatrix(predict(svmfit_best_mnist, mnist_27$test), mnist_27$test$y) |>
  simpler_cm()

# Put the test data into the proper format
mnist_27_test <- as.data.frame(mnist_27$test)
colnames(mnist_27_test) <- c("class", "x", "y")

predicted_labels_knn_mnist_test <- predict(knn_model_mnist,
                                           newdata = mnist_27_test,
                                           type = "class")

# Create a confusion matrix to evaluate the KNN model
noquote("Testing Dataset KNN Confusion Matrix:")
confusionMatrix(predicted_labels_knn_mnist_test, mnist_27$test$y) |>
  simpler_cm()
```

In conclusion, this analysis has shown that the RBF kernel has again done the best job at classifying the data, this time both on the training and test data even if only by a small margin. This again shows the power of the RBF kernel on non-linear data sets for classification.

## Conclusion

From the examples of the generated data set and the MNIST data set we can see that the RBF kernel is a powerful tool for classifying non-linear data sets. It was able to classify the generated data of the spiral with a decision boundary very close to the true decision boundary and an accuracy of 75.6% on the test data. This was better than the linear kernel, polynomial kernel, logistic regression model, and KNN model. It was also able to classify the MNIST data set of 2's and 7's with an accuracy of 83.5% on the test data. The KNN model was a close second with an accuracy of 82% on the test data. One downside of the RBF kernel is that it is computationally expensive and can take a long time to train on large data sets. It is also crucial to perform cross validation to find the best values for cost and gamma since the default values are not always the best which further increases the computational cost. However once the model is trained it is very fast at classifying new data points and does a great job at generalizing to new data assuming the chosen parameters are good. Overall the RBF kernel in support vector machines is a great tool for classifying non-linear data sets and a tool every data scientist should have in their toolbox.
