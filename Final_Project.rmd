---
title: "MATH 3190 Final Project"
author: "Jun Hanvey, Ian McFarlane, Kellen Nankervis"
date: "Due April 25, 2024"
output: 
 #beamer_presentation:
 #  theme: CambridgeUS
 pdf_document:
header-includes:
  - \usepackage{amsmath}
  - \usepackage{algorithm}
  - \newcommand{\Ab}{{\mathbf A}}
  - \newcommand{\Xb}{{\mathbf X}}
  - \newcommand{\xb}{\boldsymbol{x}}
  - \newcommand{\yb}{\boldsymbol{y}}
  - \newcommand{\bb}{\boldsymbol{b}}
  - \newcommand{\R}{\mathbb{R}}
  - \newcommand{\rb}{\boldsymbol{r}}
  - \newcommand{\ub}{\boldsymbol{u}}
  - \newcommand{\vb}{\boldsymbol{v}}
  - \newcommand{\pb}{\boldsymbol{p}}
  - \newcommand{\Ib}{{\mathbf I}}
  - \makeatletter
  - \preto{\@verbatim}{\topsep=-10pt \partopsep=4pt }
  - \makeatother
editor_options: 
  chunk_output_type: console
urlcolor: blue
---

Hello World!

```{r}
print("Hello world!")
```

## Introduction

- TODO


## Dual Problem
For the Support Vector Machine algorithm, our goal is to find a $\beta$ and $c$ under the following optimization objective:
\begin{align*}
\underset{\boldsymbol{\beta}, c}{\text{minimize }} & \Vert\boldsymbol{\beta}\Vert^2_2\\
\text{subject to } & y_i(\boldsymbol{\beta} \cdot\xb_i-c)\ge 1 \text{ for all } i
\end{align*}

For convenience, let's divide our objective function by 2, which doesn't affect the results:

\begin{align*}
\underset{\boldsymbol{\beta}, c}{\text{minimize }} & \dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2_2\\
\text{subject to } & y_i(\boldsymbol{\beta} \cdot\xb_i-c)\ge 1 \text{ for all } i
\end{align*}

Then we can find the [dual optimization problem](https://www.youtube.com/watch?v=uh1Dk68cfWs), using Lagrange Multipliers:

\begin{align*}
\underset{\boldsymbol{\lambda}}{\text{maximize }}\underset{\boldsymbol{\beta}, c}{\text{minimize }} 
L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) = \dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2_2 - \sum_{i=1}^n \lambda_i \Big( y_i(\boldsymbol{\beta} \cdot\xb_i-c) - 1 \Big) 
\end{align*}

The dual problem will be satisfied when all partial derivatives are zero, Which leads us to the following results:

\begin{align*}
\dfrac{\partial{L}}{\partial{\boldsymbol{\beta}}} & 
= \boldsymbol{\beta} - \sum_{i=1}^n \lambda_i y_i \xb_i \overset{\text{set}}{=} 0 \iff
\boldsymbol{\beta} = \sum_{i=1}^n \lambda_i y_i \xb_i \\
\dfrac{\partial{L}}{\partial{\boldsymbol{\lambda}}} & = \sum_{i=1}^n y_i(\boldsymbol{\beta} \cdot\xb_i-c) - 1 \overset{\text{set}}{=} 0 \iff c = 
\dfrac{\sum_{i=1}^n y_i \boldsymbol{\beta} \cdot\xb_i-n}{\sum_{i=1}^n y_i} \\
\dfrac{\partial{L}}{\partial{c}} & = \sum_{i=1}^n \lambda_i y_i \overset{\text{set}}{=} 0 \iff \boldsymbol{\lambda} \cdot \boldsymbol{y} = 0
\end{align*}

Observe how $\boldsymbol{\beta}$ and $c$ can be derived from $\boldsymbol{\lambda}$, $\boldsymbol{y}$ and $X$. Substituting these results into the Lagrangian should lead to some nice results. Although, substituting for $c$ won't be necessary (it gets multiplied by zero). Then, replacing $\boldsymbol{\beta}$ in our Lagrangian yields:


\begin{align*}
L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) &= \dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2_2 - \sum_{i=1}^n \lambda_i \Big( y_i(\boldsymbol{\beta} \cdot\xb_i-c) - 1 \Big)  \\
&= \dfrac{1}{2}\Vert\boldsymbol{\beta}\Vert^2_2 - \sum_{i=1}^n \lambda_i y_i\boldsymbol{\beta} \cdot\xb_i + c \sum_{i=1}^n \lambda_iy_i + \sum_{i=1}^n \lambda_i \\
&= \dfrac{1}{2}\Vert\sum_{j=1}^n \lambda_j y_j \xb_j\Vert^2_2  
- \sum_{i=1}^n \left( \sum_{j=1}^n \lambda_j y_j \xb_j\right) \cdot\Big(\lambda_i y_i\xb_i \Big) 
+ c \cdot 0 + \sum_{i=1}^n \lambda_i \\
&= \dfrac{1}{2} \left( \sum_{i=1}^n \lambda_i y_i \xb_i\right) \cdot \left( \sum_{j=1}^n \lambda_j y_j \xb_j\right) 
- \sum_{i=1}^n  \sum_{j=1}^n \left(\lambda_j y_j \xb_j\right) \cdot\left(\lambda_i y_i\xb_i\right) + \sum_{i=1}^n \lambda_i\\
&= \dfrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n (\lambda_i y_i \xb_i) \cdot(\lambda_j y_j \xb_j) - \sum_{i=1}^n \sum_{j=1}^n (\lambda_i y_i \xb_i) \cdot(\lambda_j y_j \xb_j) +  \sum_{i=1}^n \lambda_i\\
&= -\dfrac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \lambda_i\lambda_jy_iy_j\xb_i \cdot\xb_j + \sum_{i=1}^n\lambda_i\\
L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) &= \sum_{i=1}^n\lambda_i-\dfrac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \lambda_i\lambda_jy_iy_j\xb_i \cdot\xb_j
\end{align*}


This shows a version of the Lagrangian that doesn't depend on $\boldsymbol{\beta}$ nor $c$, which means the optimization problem reduces to:

\begin{align*}
\underset{\boldsymbol{\lambda}}{\text{maximize }}L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) &= \sum_{i=1}^n\lambda_i-\dfrac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \lambda_i\lambda_jy_iy_j\xb_i \cdot\xb_j 
\end{align*}

## Kernel Trick

But, why did we go through all that trouble? In this form, we can observe that the Lagrangian depends  on 3 components only:

- $\lambda_i$, an optimization artifact 
- $y_i$, a fixed "binary" variable 
- $\xb_i$, the features we're using to predict the class


Notice, $\xb_i$ is the only aspect of our model we can modify (via feature engineering). So, let $\phi(\xb)$ be our feature engineer transformation, then our Lagrangian becomes:
\begin{align*}
L(\boldsymbol{\beta}, c, \boldsymbol{\lambda}) &= \sum_{i=1}^n\lambda_i -\dfrac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \lambda_i\lambda_jy_iy_j\phi(\xb_i) \cdot\phi(\xb_j) 
\end{align*}

From this we can observe that we don't really need to calculate $\phi(\xb)$, a function that finds $\phi(\xb_i) \cdot\phi(\xb_j)$ from $\xb_i$ and $\xb_j$ is good enough. We call that function a Kernel and denote it by $K(\xb_i, \xb_j)$. To avoid having complicated sub-indices, we'll relabel the arguments of $K$ to $\boldsymbol{a}$ and $\boldsymbol{b}$, so we'll have $K(\boldsymbol{a}, \boldsymbol{b})$. Finding the $\phi(\boldsymbol{a}) \cdot\phi(\boldsymbol{b})$, without having to calculate $\phi$  is what we call the **Kernel Trick**.

![](figs/kernel_trick2.png)

One of the simplest transformations we can investigate is $K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = (\boldsymbol{a} \cdot \boldsymbol{b})^2$

If we expand the dot product, we get: 

$K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = (a_1b_1 + a_2b_2 + \cdots + a_nb_n)^2$

Expanding the square, gives:
\begin{align*}
K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = & (a_1b_1)(a_1b_1) + (a_1b_1)(a_2b_2) + \cdots + (a_1b_1)(a_nb_n) + \\
& (a_2b_2)(a_1b_1) + (a_2b_2)(a_2b_2) + \cdots + (a_2b_2)(a_nb_n) + \\
& \hspace{1in} \vdots\\
& (a_nb_n)(a_1b_1) + (a_nb_n)(a_2b_2) + \cdots + (a_nb_n)(a_nb_n)\\\\
= & (a_1a_1)(b_1b_1) + (a_1a_2)(b_1b_2) + \cdots + (a_1a_n)(b_1b_n) + \\
& (a_2a_1)(b_2b_1) + (a_2a_2)(b_2b_2) + \cdots + (a_2a_n)(b_2b_n) + \\
& \hspace{1in} \vdots\\
& (a_na_1)(b_nb_1) + (a_na_2)(b_nb_2) + \cdots + (a_na_n)(b_nb_n)
\end{align*}

Observe that the expanded sum is equivalent to the dot product of the vectors containing all pair-wise interaction terms. So, in this case 

$$\phi_{Power(2)}(\boldsymbol{x}) = 
\begin{bmatrix}
x_1x_1 \\
x_1x_2 \\
\vdots \\ 
x_1x_n \\ 
x_2x_1 \\
x_2x_2 \\
\vdots \\ 
x_2x_n \\ 
\vdots \\
x_nx_1 \\
x_nx_2 \\
\vdots \\
x_nx_n
\end{bmatrix}
$$

and $K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = \phi_{Power(2)}(\boldsymbol{a}) \cdot \phi_{Power(2)}(\boldsymbol{b}) = (\boldsymbol{a} \cdot \boldsymbol{b})^2$. However, notice we don't have to compute $\phi_{Power(2)}$ if we take $K_{Power(2)}(\boldsymbol{a}, \boldsymbol{b}) = (\boldsymbol{a} \cdot \boldsymbol{b})^2$ instead.

Likewise, one can show that: 
\begin{align*}
K_{Power(3)}(\boldsymbol{a}, \boldsymbol{b}) = & (\boldsymbol{a} \cdot \boldsymbol{b})^3 \text{ corresponds to the transformation containing all 3-way interaction terms}\\
K_{Power(4)}(\boldsymbol{a}, \boldsymbol{b}) = & (\boldsymbol{a} \cdot \boldsymbol{b})^4 \text{ corresponds to the transformation containing all 4-way interaction terms}\\
&\hspace{1in}\vdots\\
K_{Power(n)}(\boldsymbol{a}, \boldsymbol{b}) = & (\boldsymbol{a} \cdot \boldsymbol{b})^n \text{ corresponds to the transformation containing all n-way interaction terms}
\end{align*}

However, in Applied Statistics we learned that whenever we include high order terms, we also want to include all lower level terms. So, let's inspect the following kernel:

$K_{Poly(2)}(\boldsymbol{a}, \boldsymbol{b}) = (\boldsymbol{a} \cdot \boldsymbol{b} + 1)^2 = \underset{\text{2-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^2} + \underset{\text{1-way}}{2(\boldsymbol{a} \cdot \boldsymbol{b})} + \underset{\text{0-way}}{1}$ 

So, $K_{Poly(2)}$ gives us the 2-way interaction terms and all the lower order terms. Likewise,  $K_{Poly(n)}= (\boldsymbol{a} \cdot \boldsymbol{b} + 1)^n$ gives us the n-way interaction terms and below, precisely what we wanted. This is also the polynomial kernel for SVM with $\gamma$ = 1.

This proposes an interesting conundrum: which n should we pick? We could try using using cross-validation. However, our friend ***Taylor*** might have a way of trying out all n values at the same time, while giving more weight to lower order terms than the higher order terms (following the principle of parsimony).

Recall:

$\operatorname{exp}(x) = 1 + \dfrac{1}{1!}x^1 + \dfrac{1}{2!}x^2 + \dfrac{1}{3!}x^3 + \dfrac{1}{4!}x^4 + \cdots$


Then by plugging $\boldsymbol{a} \cdot \boldsymbol{b}$ for $x$, we get:

$\operatorname{exp}(\boldsymbol{a} \cdot \boldsymbol{b}) = \underset{\text{0-way}}{1} + 
\dfrac{1}{1!}\underset{\text{1-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^1} + 
\dfrac{1}{2!}\underset{\text{2-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^2} + 
\dfrac{1}{3!}\underset{\text{3-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^3} + 
\dfrac{1}{4!}\underset{\text{4-way}}{(\boldsymbol{a} \cdot \boldsymbol{b})^4} + \cdots$

Which gives us ALL n-way interaction terms, weighing the lower terms more and the higher terms less. This also means we're essentially computing the dot product of an infinite-dimensional transformation, without having to compute infinite transformations. However, to get to the Radial Basis Function, we need a couple transformations:

First, let's multiply it by $\operatorname{exp}\left(-\dfrac{\Vert\boldsymbol{a}\Vert^2_2 + \Vert\boldsymbol{b}\Vert^2_2}{2}\right)$:

\begin{align*}
\operatorname{exp}\Big(\boldsymbol{a} \cdot \boldsymbol{b}\Big)\operatorname{exp}\left(-\dfrac{\Vert\boldsymbol{a}\Vert^2_2 + \Vert\boldsymbol{b}\Vert^2_2}{2}\right) &= 
\operatorname{exp}\left(\boldsymbol{a} \cdot \boldsymbol{b}-\dfrac{\Vert\boldsymbol{a}\Vert^2_2 + \Vert\boldsymbol{b}\Vert^2_2}{2}\right)\\
&= \operatorname{exp}\left(-\dfrac{\Vert\boldsymbol{a}\Vert^2_2 + \Vert\boldsymbol{b}\Vert^2_2- 2 \boldsymbol{a} \cdot \boldsymbol{b}}{2}\right)\\
&= \operatorname{exp}\left(-\dfrac{1}{2}\Vert\boldsymbol{a}-\boldsymbol{b}\Vert^2_2\right)
\end{align*}

Now, let's raise it to the $2\gamma$ to add a control parameter.

$\left(\operatorname{exp}\left(-\dfrac{1}{2}\Vert\boldsymbol{a}-\boldsymbol{b}\Vert^2_2\right)\right)^{2\gamma} = \operatorname{exp}\Big(-\gamma \Vert\boldsymbol{a}-\boldsymbol{b}\Vert^2_2\Big)$


Finally, after 4 pages, we have arrived to the Radial Basis Function Kernel:

$K_{RBF}(\boldsymbol{a}, \boldsymbol{b}) = \operatorname{exp}\Big(-\gamma \Vert\boldsymbol{a}-\boldsymbol{b}\Vert^2_2\Big)$

In essence RBF is so special because it performs the optimization over an infinite-dimensional feature space. All that with a really simple formula, which allows for very intricate decision boundaries with minimal computational power.


## Example Data
- TODO: Find good value for n and fix colors so they are the same as the later plot. - Kellen Nankervis
To show the power of the Radial Basis Function we will first use a generated data set.

First we will generate the data. For this example I am creating a sort of spiral pattern using 1000 data points. I'm using this pattern because it is a case where a linear kernel would not work well. The data is generated in polar coordinates and then converted to x and y coordinates. The data is then offset a bit so there is some overlap between the two classes.

```{r}
set.seed(123)
n <- 1000
for (i in 1:10) {
  r <- runif(n, 1, 6 * pi + 1)
  theta <- runif(n, 0, 2 * pi)
}
# Make a sample classification n observations long
class <- sample(c(0, 1), n, replace = TRUE)
# Now fill the classification vector with the correct values
for (j in 1:n) {
  if ((r[j] + theta[j]) %% (2 * pi) < pi) {
    class[j] <- 1
  } else {
    class[j] <- 0
  }
}

# Create a data frame with the data
data <- data.frame(r, theta, class)
# Create a scatter plot of the data in x and y coordinates
data$color <- ifelse(data$class == 1, "red", "blue")
data$x <- data$r * cos(data$theta)
data$y <- data$r * sin(data$theta)

# Now offset the data a bit so there is some overlap
for (j in 1:n) {
  r <- runif(1, 0, 2)
  theta <- runif(1, 0, 2 * pi)
  # Convert to x and y coordinates
  data$x[j] <- data$x[j] + r * cos(theta)
  data$y[j] <- data$y[j] + r * sin(theta)
  # Convert the new x and y coordinates back to r and theta
  data$r[j] <- sqrt(data$x[j]^2 + data$y[j]^2)
  data$theta[j] <- atan2(data$y[j], data$x[j])
}
```
- TODO: Decide which plots to show and which to delete. Commented out ones would be my current suggestions to delete or at least move to later in the document. Make plots look good when knitted to pdf and/or slides. - Kellen Nankervis

Now lets take a look at this generated data.

```{r}
# Plot the data in the x and y coordinates
plot(data$x, data$y, col = data$color, pch = 19, xlab = "x", ylab = "y")

# Plot the data in polar coordinates
# plot(data$r, data$theta, col = data$color, pch = 19, xlab = "r", ylab = "theta")

# Plot r*theta vs. r^2 * theta^2
# plot(data$r + data$theta, data$r * data$theta, col = data$color, pch = 19, xlab = "r*theta", ylab = "r^2 * theta^2")
```

As we can see it matches a spiral pattern with a bit of overlap between the two classes. Since we know how the data was genreated it might be smart to convert to polar coordinates, but if this data weren't generated we might not make that connection. This is where the RBF kernel can be very useful.

- TODO: Decide on good cost to not over fit. Could be a good spot to also show how we can use cross-validation to find the best cost. Make plots look good when knitted to pdf and/or slides. - Kellen Nankervis

Now lets use the RBF kernel to classify this data. Right now we will use a cost of 10 and a gamma of 10, but later we can use cross-validation to find the best values for these parameters.

```{r}
# Load the required svm library
library(e1071)
library(caret)

# Convert class to a factor
data$class <- as.factor(data$class)

# Create a data frame with only the class and the x and y coordinates
data2 <- data.frame(class = data$class, x = data$x, y = data$y)
data2$class <- as.factor(data2$class)

# Use a radial basis function kernel to classify the data with the SVM function
svmfit <- svm(class ~ ., data = data2, kernel = "radial", cost = 10, gamma = 10)

print(svmfit)

# Create a confusion matrix to evaluate the SVM model
confusionMatrix(predict(svmfit, data2), data2$class)
```

From the confusion matrix we can see that the model is doing a descent job at classifying the data, with an accuracy of 85.5%. Now let's look at the decision boundary to see how it is classifying the data.

```{r}
# Define colors for data points
point_colors <- c("blue", "red")

# Define colors for decision boundary
boundary_colors <- c("skyblue", "orange")

# Plot the data points
plot(data2$x, data2$y, col = point_colors[data2$class], pch = 19, xlab = "x", ylab = "y")

# Plot the decision boundary
x1_grid <- seq(min(data2$x), max(data2$x), length.out = 100)
x2_grid <- seq(min(data2$y), max(data2$y), length.out = 100)
grid <- expand.grid(x = x1_grid, y = x2_grid)

predicted_labels <- predict(svmfit, newdata = grid)

plot(grid$x, grid$y, col = boundary_colors[predicted_labels], pch = ".", cex = 3.5, xlab = "x", ylab = "y")

# Plot the data points
points(data2$x, data2$y, col = point_colors[data2$class], pch = 19)
legend("topright", legend = levels(data2$class), col = point_colors, pch = 19)
```

- If we want to plot the decision boundary above the points we can use the following code. - Kellen Nankervis
```{r}
# Plot the data points
plot(data2$x, data2$y, col = point_colors[data2$class], pch = 19, xlab = "x", ylab = "y")

# Plot the decision boundary
points(grid$x, grid$y, col = boundary_colors[predicted_labels], pch = ".", cex = 3.5)
legend("topright", legend = levels(data2$class), col = point_colors, pch = 19)
```

- Or to just plot the decision boundary we can use the following code. - Kellen Nankervis
```{r}
# Plot the decision boundary
plot(grid$x, grid$y, col = boundary_colors[predicted_labels], pch = ".", cex = 3.5, xlab = "x", ylab = "y")
legend("topright", legend = levels(data2$class), col = point_colors, pch = 19)
```

We can see that the decision boundary is doing a good job at classifying the data. The boundary isn't quite perfect, since it doesn't maintain the same width throughout the spiral, and it also has some gaps, but overall it is doing a good job at classifying the data and finding the spiral pattern used to generate it in this example.

- TODO: Use cross validation to find the best cost and gamma values. - Kellen Nankervis

- Compare to a linear kernel and a logistic regression model without many interactions. - Kellen Nankervis

- Example of how to plot I found online. Should be removed for final paper but I thought it might be helpful to look at. - Kellen Nankervis
```{r}
print("")

# Create a toy dataset
set.seed(123)
data <- data.frame(
  x1 = rnorm(50, mean = 2),
  x2 = rnorm(50, mean = 2),
  label = c(rep("Red", 25), rep("Blue", 25)) |> as.factor()
)

# Train an SVM
svm_model <- svm(label ~ ., data = data, kernel = "radial")

# Create a grid of points for prediction
x1_grid <- seq(min(data$x1), max(data$x1), length.out = 100)
x2_grid <- seq(min(data$x2), max(data$x2), length.out = 100)
grid <- expand.grid(x1 = x1_grid, x2 = x2_grid)

# Predict class labels for the grid
predicted_labels <- predict(svm_model, newdata = grid)

# Plot the decision boundary
plot(data$x1, data$x2, col = factor(data$label), pch = 19, main = "SVM Decision Boundary")
points(grid$x1, grid$x2, col = factor(predicted_labels), pch = ".", cex = 2.5)
legend("topright", legend = levels(data$label), col = c("blue", "red"), pch = 19)
```
